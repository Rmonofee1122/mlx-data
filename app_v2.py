from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles  # â† ã“ã‚Œã‚‚å¿…è¦ï¼
from mlx_lm import load, generate
from mlx_lm.sample_utils import make_sampler
import subprocess
import time
import os
import json
import asyncio

app = FastAPI()

# ğŸ”§ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã¨LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
t0 = time.time()
model, tokenizer = load(
    "/Users/neo-aichan/models/Llama-3.3-Swallow-70B-v0.4",
    adapter_path="/Users/neo-aichan/models/llama3.3-swallow-70b-mlx-adapter"
)
print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {time.time() - t0:.2f}ç§’")

# âœ… mount ã¯ app å®šç¾©ã®ã‚ã¨ã«æ›¸ãï¼
app.mount("/static", StaticFiles(directory="static"), name="static")

# âœ… /chat ãƒ«ãƒ¼ãƒˆ
@app.get("/chat", response_class=HTMLResponse)
async def serve_chat():
    with open("chat.html", encoding="utf-8") as f:
        return f.read()

# temperatureã‚’è¨­å®šã—ãŸsamplerã‚’ä½œæˆ
sampler = make_sampler(
    temp=1.4,
    top_p=0.9,
    )        

# âœ… /generate API
class PromptRequest(BaseModel):
    prompt: str
    max_tokens: int = 300
    num_return_sequences: int = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§3æ¡ˆç”Ÿæˆ
    stream: bool = False  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã‚’åˆ¶å¾¡ã™ã‚‹ãƒ•ãƒ©ã‚°

# ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«ç”Ÿæˆã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿é–¢æ•°
async def generate_tokens(prompt, max_tokens, sequence_idx=0):
    wrapped_prompt = f"<s>[INST] {prompt} [/INST]"
    print(f"\U0001f4e5 Prompt: {wrapped_prompt}")
    
    # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“
    full_text = ""
    
    # generateé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
    response = generate(
        model=model,
        tokenizer=tokenizer,
        prompt=wrapped_prompt,
        max_tokens=max_tokens,
        sampler=sampler,
        verbose=False,
    )
    
    # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’1æ–‡å­—ãšã¤ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
    for i in range(len(response)):
        # 1æ–‡å­—ãšã¤å–å¾—
        delta = response[i]
        full_text += delta
        
        # æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã‚’è¿”ã™
        yield json.dumps({"delta": delta, "sequence_idx": sequence_idx}) + "\n"
        
        # å°‘ã—å¾…æ©Ÿã—ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åŠ¹æœã‚’å¼·èª¿
        await asyncio.sleep(0.01)
    
    print(f"\U0001f4e4 Response {sequence_idx+1}: {full_text}")

# âœ… æ¨è«–APIï¼ˆè¤‡æ•°æ¡ˆå¯¾å¿œï¼‰
@app.post("/generate")
async def generate_text(req: PromptRequest):
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ãŒç„¡åŠ¹ã®å ´åˆã¯å¾“æ¥ã®æ–¹æ³•ã§ç”Ÿæˆ
    if not req.stream:
        wrapped_prompt = f"<s>[INST] {req.prompt} [/INST]"
        print(f"\U0001f4e5 Prompt: {wrapped_prompt}")

        results = []
        for i in range(req.num_return_sequences):
            response = generate(
                model=model,
                tokenizer=tokenizer,
                prompt=wrapped_prompt,
                max_tokens=req.max_tokens,
                sampler=sampler,
                verbose=False,
            )
            print(f"\U0001f4e4 Response {i+1}: {response}")
            results.append(response)

        return {"responses": results}
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ã®å ´åˆã¯StreamingResponseã‚’è¿”ã™
    async def stream_generator():
        for i in range(req.num_return_sequences):
            async for token in generate_tokens(req.prompt, req.max_tokens, i):
                yield token
        
        # ç”Ÿæˆå®Œäº†ã‚’ç¤ºã™çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        yield json.dumps({"finish": True}) + "\n"
    
    return StreamingResponse(
        stream_generator(),
        media_type="application/x-ndjson"
    )

# âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å°‚ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/generate_stream")
async def generate_stream(req: PromptRequest):
    async def stream_generator():
        for i in range(req.num_return_sequences):
            async for token in generate_tokens(req.prompt, req.max_tokens, i):
                yield token
        
        # ç”Ÿæˆå®Œäº†ã‚’ç¤ºã™çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        yield json.dumps({"finish": True}) + "\n"
    
    return StreamingResponse(
        stream_generator(),
        media_type="application/x-ndjson"
    )
