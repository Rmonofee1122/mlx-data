from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles  # â† ã“ã‚Œã‚‚å¿…è¦ï¼
from mlx_lm import load, generate
import subprocess
import time
import os

app = FastAPI()

# ğŸ”§ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã¨LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
t0 = time.time()
model, tokenizer = load(
    "/Users/neo-aichan/models/Llama-3.3-Swallow-70B-v0.4",
    adapter_path="/Users/neo-aichan/models/llama3.3-swallow-70b-mlx-adapter"
)
print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {time.time() - t0:.2f}ç§’")

# âœ… mount ã¯ app å®šç¾©ã®ã‚ã¨ã«æ›¸ãï¼
app.mount("/static", StaticFiles(directory="static"), name="static")

# âœ… /chat ãƒ«ãƒ¼ãƒˆ
@app.get("/chat", response_class=HTMLResponse)
async def serve_chat():
    with open("chat.html", encoding="utf-8") as f:
        return f.read()

# âœ… /generate ãƒ«ãƒ¼ãƒˆ
class PromptRequest(BaseModel):
    prompt: str
    max_tokens: int = 100
    temperature: float = 1.4
    top_p: float = 0.9

@app.post("/generate")
async def generate_text(req: PromptRequest):

    wrapped_prompt = f"<s>[INST] {req.prompt} [/INST]"

    cmd = [
        "mlx_lm.generate",
        "--model", "/Users/neo-aichan/models/Llama-3.3-Swallow-70B-v0.4",
        "--adapter-path", "/Users/neo-aichan/models/llama3.3-swallow-70b-mlx-adapter",
        "--prompt", wrapped_prompt,
        "--max-tokens", str(req.max_tokens),
        "--temp", str(req.temperature),
        "--top-p", str(req.top_p),
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)
    return {"response": result.stdout.strip()}
