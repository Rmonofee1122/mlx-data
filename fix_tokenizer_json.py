import json

# èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
input_path = "/Users/neo-aichan/models/Llama-3.3-Swallow-70B-v0.4/tokenizer.json"
backup_path = "/Users/neo-aichan/models/Llama-3.3-Swallow-70B-v0.4/tokenizer_backup.json"
output_path = input_path  # ä¸Šæ›¸ã

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å–ã‚‹
with open(input_path, "r", encoding="utf-8") as f:
    data = f.read()

with open(backup_path, "w", encoding="utf-8") as f:
    f.write(data)

print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆæ¸ˆã¿: {backup_path}")

# èª­ã¿è¾¼ã‚“ã§ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ã“ã“ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹ï¼‰
try:
    tokenizer_json = json.loads(data)
    print("âœ… tokenizer.json ã®æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ OK")
except json.JSONDecodeError as e:
    print(f"âŒ JSON æ§‹æ–‡ã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ‘‰ æ‰‹å‹•ä¿®æ­£ã¾ãŸã¯è‡ªå‹•ä¿®æ­£ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚")
    exit(1)

# æ•´å½¢ã—ã¦æ›¸ãå‡ºã™
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(tokenizer_json, f, ensure_ascii=False, indent=2)

print(f"âœ… ä¿®æ­£æ¸ˆã¿ tokenizer.json ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")